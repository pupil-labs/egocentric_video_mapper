{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egocentric Video Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting make sure you have an available GPU. If you are unsure about whether you have an available GPU or if you want to check which GPU you will be working with run the next executble cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-72b7ddec-7074-1583-0076-9281c065ae9d)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to import the modules and functions that will be used for this AlphaLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import (\n",
    "    VideoHandler,\n",
    "    write_timestamp_csv,\n",
    "    generate_mapper_kwargs,\n",
    "    generate_comparison_video_kwargs,\n",
    ")\n",
    "from optic_flow import OpticFlowCalculatorLK\n",
    "from sync_videos import OffsetCalculator\n",
    "from gaze_mapper import RulesBasedGazeMapper\n",
    "from video_renderer import save_comparison_video, save_gaze_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path to the directory of the uncompressed Scene Video + Timeseries download from Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_timeseries_path = (\n",
    "    \"/users/sof/video_examples/second_video/2024-05-23_16-47-35-a666ea62\"\n",
    ")\n",
    "neon_vid_path = Path(neon_timeseries_path).rglob(\"*.mp4\").__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path of the corresponding alternative egocentric view recording (please make sure that both videos have the same orientation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_vid_path = \"/users/sof/video_examples/second_video/20240523_171941_000.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an output directory where the different output files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/users/sof/action_map_experiments/s_try\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Video Synchronization\n",
    "\n",
    "In this first step, optic flow in both videos will be calculated. Each optic flow result is saved to a csv file in an automatically created optic_flow subdirectory in the output directory. The csv file will contain the following columns:\n",
    "- start: the start frame timestamp of the optic flow calculation\n",
    "- end: the end frame timestamp of the optic flow calculation\n",
    "- dx: Average horizontal displacement  \n",
    "- dy: Average vertical displacement \n",
    "- angle: Average angular change in degrees\n",
    "\n",
    "Optic flow is then used to obtain the time offset of the Action video with respect to the Neon Scene video. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_of = OpticFlowCalculatorLK(video_dir=neon_vid_path)\n",
    "neon_of_result = neon_of.process_video(\n",
    "    output_file=Path(output_dir, \"optic_flow/neon_lk_of.csv\")\n",
    ")\n",
    "\n",
    "alternative_of = OpticFlowCalculatorLK(video_dir=alternative_vid_path)\n",
    "alternative_of_result = alternative_of.process_video(\n",
    "    output_file=Path(output_dir, \"optic_flow/alternative_lk_of.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once optic flow is measured, the time offset can be calculated and a action_worldtimestamps.csv will be generated with a similar format as Neon's world_timestamps.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated offset of alternative video with respect to Neon scene video: 2.016 seconds (Pearson correlation: 0.9192123717590794)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Last timestamp of alternative camera recording (2024-05-23 14:48:20.852368384) is after the last timestamp of Neon Scene recording (2024-05-23 14:48:18.721666666)\n"
     ]
    }
   ],
   "source": [
    "offset_calc = OffsetCalculator(\n",
    "    src=alternative_of_result[\"dy\"].values,\n",
    "    src_timestamps=alternative_of_result[\"start\"].values,\n",
    "    dst=neon_of_result[\"dy\"].values,\n",
    "    dst_timestamps=neon_of_result[\"start\"].values,\n",
    "    resampling_frequency=500,\n",
    ")\n",
    "\n",
    "t_offset, pearson_corr = offset_calc.estimate_time_offset()\n",
    "print(\n",
    "    f\"Estimated offset of alternative video with respect to Neon scene video: {t_offset} seconds (Pearson correlation: {pearson_corr})\"\n",
    ")\n",
    "\n",
    "write_timestamp_csv(\n",
    "    neon_timeseries_path,\n",
    "    VideoHandler(alternative_vid_path).timestamps + t_offset,\n",
    "    saving_path=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Map gaze data to the action video\n",
    "\n",
    "After synchronizing both videos, it is time to obtain Neon gaze signal in the coordinate system of the other video. \n",
    "\n",
    "Here we make use of deep learning methods to guide the gaze transformation, the deafault in this notebook is uses an implementation of Efficient LOFTR.\n",
    "\n",
    "We have implemented other algorithms: 'LOFTR', 'DISK_LightGlue', 'DeDoDe_LightGlue'. If you wish to try them out, write the name of the desired algorithm in the `image_matcher_choice` variable in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_matcher_choice = \"Efficient_LOFTR\"  # Options: 'Efficient_LOFTR', 'LOFTR', 'DISK_LightGlue', 'DeDoDe_LightGlue'\n",
    "\n",
    "mapper_kwargs = generate_mapper_kwargs(\n",
    "    neon_timeseries_path, alternative_vid_path, output_dir, image_matcher_choice\n",
    ")\n",
    "\n",
    "mapper = RulesBasedGazeMapper(**mapper_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the mapping pipeline you can adjust how often (in seconds) you want the image correspondances refreshed. \n",
    "\n",
    "- The higher you set the time threshold the less time the mapping will take however it won't be as accurate as it could be. \n",
    "\n",
    "- The lower the time threshold the more time the mapping will take. \n",
    "\n",
    "If you leave the value at 'None' then for every gaze new imge correspondences will be obtained. We recommend using values smaller than 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_kwargs = {\n",
    "    \"saving_path\": Path(\n",
    "        output_dir,\n",
    "        f\"mapped_gaze/{image_matcher_choice.lower()}/alternative_camera_gaze_lk.csv\",\n",
    "    ),\n",
    "    \"refresh_time_thrshld\": time_threshold,\n",
    "}\n",
    "mapped_gaze_path = mapper.map_gaze(**mapping_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 (OPTIONAL): Create visualization videos\n",
    "\n",
    "For visualization purposes you can get a side-to-side rendering with gaze of the Neon Scene video and the alternative egocentric video.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_video_kwargs = generate_comparison_video_kwargs(\n",
    "    neon_timeseries_path,\n",
    "    alternative_vid_path,\n",
    "    mapped_gaze_path,\n",
    "    output_dir,\n",
    "    image_matcher_choice,\n",
    ")\n",
    "save_comparison_video(**comparison_video_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can get only the alternative egocentric video (at its original frame rate) with the overlaid gaze circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_video_args = {\n",
    "    \"video_path\": alternative_vid_path,\n",
    "    \"timestamps_path\": Path(output_dir, \"action_camera_timestamps.csv\"),\n",
    "    \"gaze_path\": Path(mapped_gaze_path),\n",
    "    \"save_video_path\": Path(\n",
    "        output_dir, f\"rendered_videos/{image_matcher_choice}_lk.avi\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "save_gaze_video(**gaze_video_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sof_gazemapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
