{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egocentric Video Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting make sure you have an available GPU. If you are unsure about whether you have an available GPU or if you want to check which GPU you will be working with run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will import the modules and functions that will be used for this Alpha Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import (\n",
    "    VideoHandler,\n",
    "    write_timestamp_csv,\n",
    "    generate_mapper_kwargs,\n",
    "    generate_comparison_video_kwargs,\n",
    ")\n",
    "from optic_flow import OpticFlowCalculatorLK\n",
    "from sync_videos import OffsetCalculator\n",
    "from gaze_mapper import EgocentricMapper\n",
    "from video_renderer import save_comparison_video, save_gaze_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path to the directory of the uncompressed `Timeseries Data + Scene Video` download from Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_timeseries_dir = \"Path/To/NeonTimeSeriesFolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path of the corresponding alternative egocentric view recording (please make sure that both videos have the same orientation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_vid_path = \"Path/To/AlternativeVideo.ext\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an output directory where the different output files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Path/To/OutputFolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Video Synchronization\n",
    "\n",
    "In this first step, optic flow in both videos will be calculated. Each optic flow result is saved to a csv file in an automatically created optic_flow subdirectory in the output directory. The csv file will contain the following columns:\n",
    "- start: The start frame timestamp of the optic flow calculation\n",
    "- end: The end frame timestamp of the optic flow calculation\n",
    "- dx: Average horizontal displacement  \n",
    "- dy: Average vertical displacement \n",
    "- angle: Average angular change in degrees\n",
    "\n",
    "Optic flow is then used to obtain the time offset of the alternative egocentric video with respect to the Neon Scene video. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_vid_path = Path(neon_timeseries_dir).rglob(\"*.mp4\").__next__()\n",
    "neon_of = OpticFlowCalculatorLK(video_path=neon_vid_path)\n",
    "neon_of_result = neon_of.process_video(\n",
    "    output_file_path=Path(output_dir, \"optic_flow/neon_lk_of.csv\")\n",
    ")\n",
    "\n",
    "alternative_of = OpticFlowCalculatorLK(video_path=alternative_vid_path)\n",
    "alternative_of_result = alternative_of.process_video(\n",
    "    output_file_path=Path(output_dir, \"optic_flow/alternative_lk_of.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once optic flow is measured, the time offset will be calculated and an alternative_camera_timestamps.csv will be generated with a similar format as Neon's world_timestamps.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_calc = OffsetCalculator(\n",
    "    src=alternative_of_result[\"dy\"].values,\n",
    "    src_timestamps=alternative_of_result[\"start\"].values,\n",
    "    dst=neon_of_result[\"dy\"].values,\n",
    "    dst_timestamps=neon_of_result[\"start\"].values,\n",
    "    resampling_frequency=500,\n",
    ")\n",
    "\n",
    "t_offset, pearson_corr = offset_calc.estimate_time_offset()\n",
    "print(\n",
    "    f\"Estimated offset of alternative egocentric video with respect to Neon scene video: {t_offset} seconds (Pearson correlation: {pearson_corr})\"\n",
    ")\n",
    "\n",
    "write_timestamp_csv(\n",
    "    neon_timeseries_dir,\n",
    "    VideoHandler(alternative_vid_path).timestamps + t_offset,\n",
    "    output_file_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Map gaze data to the action video\n",
    "\n",
    "After synchronizing both videos, it is time to obtain Neon gaze signal in the coordinate system of the other video. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Here we make use of deep learning methods to guide the gaze mapping. By default, this notebook uses an implementation of Efficient LOFTR. We have implemented the next other algorithms: 'LOFTR', 'DISK_LightGlue', 'DeDoDe_LightGlue'. If you wish to try them out, just write the name of the desired algorithm in the <b>image_matcher_choice</b> variable found in the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_matcher_choice = \"Efficient_LOFTR\"  # Options: 'Efficient_LOFTR', 'LOFTR', 'DISK_LightGlue', 'DeDoDe_LightGlue'\n",
    "\n",
    "mapper_kwargs = generate_mapper_kwargs(\n",
    "    neon_timeseries_dir, alternative_vid_path, output_dir, image_matcher_choice\n",
    ")\n",
    "\n",
    "mapper = EgocentricMapper(**mapper_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the mapping pipeline you can adjust how often (in seconds) you want the image correspondances refreshed. \n",
    "\n",
    "\n",
    "- The higher you set the time threshold the less time the mapping will take, however it may decrease the accuracy of the mapping. \n",
    "- The lower the time threshold the more time the mapping will take. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "If you leave the value at 'None' then for every gaze new correspondences will be obtained. \n",
    "We recommend using values smaller than 0.5 seconds.\n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_kwargs = {\n",
    "    \"saving_path\": Path(\n",
    "        output_dir,\n",
    "        f\"mapped_gaze/{image_matcher_choice.lower()}/alternative_camera_gaze_lk.csv\",\n",
    "    ),\n",
    "    \"refresh_time_thrshld\": time_threshold,\n",
    "}\n",
    "mapped_gaze_path = mapper.map_gaze(refresh_time_thrshld=time_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 (OPTIONAL): Create visualization videos\n",
    "\n",
    "For visualization purposes you can get a side-to-side rendering with gaze of the Neon Scene video and the alternative egocentric video.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_video_kwargs = generate_comparison_video_kwargs(\n",
    "    neon_timeseries_dir,\n",
    "    alternative_vid_path,\n",
    "    mapped_gaze_path,\n",
    "    output_dir,\n",
    "    image_matcher_choice,\n",
    ")\n",
    "save_comparison_video(**comparison_video_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can get only the alternative egocentric video (at its original frame rate) with the overlaid gaze circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_video_args = {\n",
    "    \"video_path\": alternative_vid_path,\n",
    "    \"timestamps_path\": Path(output_dir, \"alternative_camera_timestamps.csv\"),\n",
    "    \"gaze_path\": Path(mapped_gaze_path),\n",
    "    \"save_video_path\": Path(\n",
    "        output_dir, f\"rendered_videos/{image_matcher_choice}_lk.mp4\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "save_gaze_video(**gaze_video_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sof_gazemapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
