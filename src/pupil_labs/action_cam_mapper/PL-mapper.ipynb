{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egocentric Video Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting make sure you have an available GPU. If you are unsure about whether you have an available GPU or if you want to check which GPU you will be working with run the next executble cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-72b7ddec-7074-1583-0076-9281c065ae9d)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to import the modules and functions that will be used for this AlphaLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from utils import VideoHandler, write_action_timestamp_csv\n",
    "from optic_flow import OpticFlowCalculatorLK\n",
    "from sync_videos import OffsetCalculator\n",
    "from gaze_mapper import RulesBasedGazeMapper\n",
    "from video_renderer import save_comparison_video, save_gaze_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path to the directory of the uncompressed Scene Video + Timeseries download from Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_timeseries_path='Path/To/NeonTimeSeriesFolder'\n",
    "neon_vid_path= Path(neon_timeseries_path).rglob('*.mp4').__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path of the corresponding alternative egocentric view recording (please make sure that both videos have the same orientation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_vid_path='Path/To/AlternativeVideo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an output directory where the different output files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='Path/To/OutputFolder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Video Synchronization\n",
    "\n",
    "In this first step, optic flow in both videos will be calculated. Each optic flow result is saved to a csv file in an automatically created optic_flow subdirectory in the output directory. The csv file will contain the following columns:\n",
    "- start: the start frame timestamp of the optic flow calculation\n",
    "- end: the end frame timestamp of the optic flow calculation\n",
    "- dx: Average horizontal displacement  \n",
    "- dy: Average vertical displacement \n",
    "- angle: Average angular change in degrees\n",
    "\n",
    "Optic flow is then used to obtain the time offset of the Action video with respect to the Neon Scene video. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File already exists, appending to it\n",
      "File already exists, appending to it\n"
     ]
    }
   ],
   "source": [
    "optic_flow_output_dir = Path(output_dir, 'optic_flow')\n",
    "optic_flow_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "neon_of = OpticFlowCalculatorLK(video_dir=neon_vid_path)\n",
    "neon_result = neon_of.process_video(output_file=Path(optic_flow_output_dir, 'neon_lk_of.csv'))\n",
    "\n",
    "action_of = OpticFlowCalculatorLK(video_dir=action_vid_path)\n",
    "action_result = action_of.process_video(output_file=Path(optic_flow_output_dir, 'action_lk_of.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the time offset is calculated, the action_worldtimestamps.csv can be generated with a similar format as the Neon world_timestamps.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Last timestamp of Action camera recording (2024-05-23 14:48:20.852368384) is after the last timestamp of Neon Scene recording (2024-05-23 14:48:18.721666666)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated offset: 2.016 seconds (Pearson correlation: 0.9192123717590794)\n"
     ]
    }
   ],
   "source": [
    "offset_calc=OffsetCalculator(src=action_result['dy'].values,src_timestamps=action_result['start'].values, dst=neon_result['dy'].values, dst_timestamps=neon_result['start'].values,resampling_frequency=500)\n",
    "\n",
    "t_offset, pearson_corr = offset_calc.estimate_time_offset()\n",
    "print(f'Estimated offset: {t_offset} seconds (Pearson correlation: {pearson_corr})')\n",
    "\n",
    "write_action_timestamp_csv(neon_timeseries_path, VideoHandler(action_vid_path).timestamps+t_offset, saving_path=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Map gaze data to the action video\n",
    "\n",
    "After synchronizing both videos, it is time to obtain Neon gaze signal in the coordinate system of the other video. Here we make use of deep learning methods to guide the gaze transformation, for this notebook we will be using an implementation of Efficient LOFTR, if you are interested in using other algorithms we have implemented or in implementing one yourself check feature_matcher.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_matcher_choice='Efficient_LOFTR' # Options: 'Efficient_LOFTR', 'LOFTR', 'DISK_LightGlue', 'DeDoDe_LightGlue'\n",
    "\n",
    "\n",
    "image_matcher_parameters = {'Efficient_LOFTR': {'model_type': 'opt', 'gpu_num': 0},'LOFTR':{\"location\": \"indoor\", \"gpu_num\": 0},'DISK_LightGlue': {\"num_features\": 2048, \"gpu_num\": 0},'DeDoDe_LightGlue': {\"num_features\": 5000, \"gpu_num\": 0}}\n",
    "optic_flow_output_dir = Path(output_dir, 'optic_flow')\n",
    "mapper_kwargs = {'neon_gaze_csv': Path(neon_timeseries_path, 'gaze.csv'),\n",
    "                'neon_video_dir': neon_vid_path,\n",
    "                'action_video_dir': action_vid_path,\n",
    "                'neon_timestamps':  Path(neon_timeseries_path, 'world_timestamps.csv'),\n",
    "                'action_timestamps': Path(output_dir, 'action_camera_timestamps.csv'),\n",
    "                'neon_opticflow_csv': Path(optic_flow_output_dir, 'neon_lk_of.csv'),\n",
    "                'action_opticflow_csv': Path(optic_flow_output_dir, 'action_lk_of.csv'),\n",
    "                'image_matcher': image_matcher_choice,\n",
    "                'image_matcher_parameters': image_matcher_parameters[image_matcher_choice],\n",
    "                'patch_size': 1000}\n",
    "\n",
    "mapper = RulesBasedGazeMapper(**mapper_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the mapping pipeline you can adjust how often (in seconds) you want the image correspondances refreshed. \n",
    "\n",
    "- The higher you set the time threshold the less time the mapping will take however it won't be as accurate as it could be. \n",
    "\n",
    "- The lower the time threshold the more time the mapping will take. \n",
    "\n",
    "If you leave the value at 'None' then for every gaze new imge correspondences will be obtained. We recommend using values smaller than 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_threshold = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_gaze_output_dir = Path(output_dir, f'mapped_gaze/{image_matcher_choice.lower()}')\n",
    "action_gaze_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "mapping_kwargs = {'saving_path':Path(action_gaze_output_dir,f\"action_gaze_lk.csv\"),'refresh_time_thrshld':time_threshold}\n",
    "mapper.map_gaze(**mapping_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 (OPTIONAL): Create visualization videos\n",
    "\n",
    "For visualization purposes you can get a side-to-side rendering with gaze of the Neon Scene video and the alternative egocentric video. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_gaze_dict = {image_matcher_choice.upper(): Path(action_gaze_output_dir,f\"action_gaze_lk.csv\")}\n",
    "rendered_video_path = Path(output_dir, f\"rendered_videos/neon_comparison_{image_matcher_choice.lower()}_lk.avi\")\n",
    "Path(rendered_video_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "comparison_video_args = {'action_video_path':action_vid_path,\n",
    "                        'action_worldtimestamps_path':Path(output_dir, 'action_camera_timestamps.csv'),\n",
    "                        'action_gaze_paths_dict':action_gaze_dict,\n",
    "                        'neon_video_path':neon_vid_path,\n",
    "                        'neon_worldtimestamps_path':neon_timestamps,\n",
    "                        'neon_gaze_path':Path(neon_timeseries_path, 'gaze.csv'),\n",
    "                        'save_video_path':rendered_video_path}\n",
    "\n",
    "save_comparison_video(**comparison_video_args)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also can get only the alternative egocentric video  (at its original frame rate) with the overlaid gaze circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_video_args={'video_path':action_vid_path,\n",
    "                'timestamps_path':Path(output_dir, 'action_camera_timestamps.csv'),\n",
    "                'gaze_path':Path(action_gaze_output_dir, 'action_gaze_lk.csv'),\n",
    "                'save_video_path':Path(output_dir, f\"rendered_videos/{image_matcher_choice}_lk.avi\")}\n",
    "save_gaze_video(**gaze_video_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sof_gazemapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
