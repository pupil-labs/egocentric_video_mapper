{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egocentric Video Mapper\n",
    "\n",
    "Before starting make sure you have an available GPU. If you are unsure about whether you have an available GPU or if you want to check which GPU you will be working with run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path to the directory of the uncompressed `Timeseries Data + Scene Video` download from Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_timeseries_dir = \"Path/To/NeonTimeSeriesFolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path of the corresponding alternative egocentric view recording (please make sure that both videos have the same orientation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_vid_path = \"Path/To/AlternativeVideo.ext\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an output directory where the different output files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Path/To/OutputFolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select whether you want to map fixations, the whole gaze signal or both signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_choice = \"Fixations\"  # 'Fixations', 'Gaze' or 'Both'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select configurations for mapping gaze\n",
    "\n",
    "These configurations only affect the mapping of the 200Hz gaze measurements found in the gaze.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control how often, in seconds, the egocentric video mapper recalculates new image matches between the videos.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">Note: Higher time thresholds lead to a reduction in computation time, however it may decrease mapping accuracy.</div>\n",
    "\n",
    "If you leave the value at 0 then for every gaze measurement new image matches will be calculated (slowest option but most accurate one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_time_threshold_sec = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "Set the `render_video` variable to True if you want to render the mapped gaze into a copy of the alternative egocentric video (at its original frame rate).\n",
    "\n",
    "Similarly, set the `render_video_comparison` variable to True if you want to render both egocentric videos (Neon Scene and the alternative egocentric camera) side by side showing their respective gaze measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_video = False\n",
    "render_video_comparison = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced configurations\n",
    "\n",
    "Here you can choose which optic flow algorithm will be used for video synchronization,as default we use Lucas-Kanade sparse algorithm. However in quasi-static videos or in videos with very feature poor scenes we recommend using a dense optic flow method, note that using a dense optic flow method will increase the computation time. \n",
    "\n",
    "You can also choose which image matcher algorithm that will guide the mapping. The publicly available Efficient LOFTR model was trained on outdoor images (MegaDepth dataset), if you would like a model specialized in indoor settings (trained on ScanNet dataset) we have available the indoor model of LOFTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optic_flow_algorithm = \"Lucas-Kanade\"  # \"Lucas-Kanade\" or \"Gunnar Farneback\"\n",
    "image_matcher = \"Efficient_LOFTR\"  # \"Efficient_LOFTR\", \"LOFTR_indoor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running the Egocentric Video Mapper, check the alternative egocentric video orientation.\n",
    "\n",
    "Please make sure the orientation of the alternative egocentric video is the same as the Neon Scene Camera Video. This view can sometimes differ from the video view in players like VLC or QuickTime due to metadata in the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pupil_labs.egocentric_video_mapper.utils import show_videos_preview\n",
    "\n",
    "show_videos_preview(neon_timeseries_dir, alternative_vid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the orientation matches in both videos, you can jump ahead and run the Egocentric Video Mapper!\n",
    "\n",
    "Otherwise, choose in the cell below the rotation needed for the correct visualization of the video and execute it. The cell will run a ffmpeg command to create an orientation-corrected video in the same folder as your original alternative egocentric video.\n",
    "\n",
    "The path to the corrected alternative egocentric video will be printed. Once the orientation looks right, update the alternative video path with this new path at the beginning of the notebook and rerun the cells above this one.\n",
    "\n",
    "**NOTE**: This command will modify the videoplayer metadata so the video is displayed in the same orientation in both the Egocentric Video Mapper and in VLC/Quick Time video players.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pupil_labs.egocentric_video_mapper.utils import execute_ffmpeg_command\n",
    "\n",
    "rotation = \"90° clockwise\"  # \"90° clockwise\",\"90° counterclockwise\", \"180°\",\"0°\"\n",
    "video_cmd, new_alternative_vid_path = execute_ffmpeg_command(\n",
    "    rotation=rotation, video_path=alternative_vid_path\n",
    ")\n",
    "\n",
    "print(f\"New alternative video path: {new_alternative_vid_path}\")\n",
    "show_videos_preview(neon_timeseries_dir, new_alternative_vid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's run the egocentric video mapper!\n",
    "\n",
    "When everything is finished, you will find the following in the specified output folder:\n",
    "- `alternative_camera_gaze.csv`: The mapped gaze signal. It follows the same structure as gaze.csv, with the frequency of the gaze signal (200Hz) being preserved. This way you can easily integrate it into your existing pipelines.\n",
    "- `alternative_camera_timestamps.csv`: Synchronized UTC timestamps for every alternative egocentric video frame. It follows the same structure as world_timestamps.csv\n",
    "- `alternative_camera-neon_comparison.mp4`: The comparison video showing side by side the Neon Scene camera video and the alternative camera video with their respective gaze signal overlaid.\n",
    "- `alternative_camera_gaze_overlay.mp4`: A copy of the alternative egocentric video with the mapped gaze overlaid.\n",
    "- `neon_optic_flow.csv`: Contains the average optic flow csv for Neon Scene video.\n",
    "- `alternative_optic_flow.csv`: Contains the average optic flow csv for alternative egocentric video.\n",
    "- `egocentric_video_mapper_args.json`: This file contains the different parameters and configurations used to map the gaze signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pupil_labs.egocentric_video_mapper.__main__ as main\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.neon_timeseries_dir = neon_timeseries_dir\n",
    "        self.alternative_vid_path = alternative_vid_path\n",
    "        self.output_dir = output_dir\n",
    "        self.mapping_choice = mapping_choice\n",
    "        self.optic_flow_choice = optic_flow_algorithm\n",
    "        self.matcher = image_matcher\n",
    "        try:\n",
    "            self.refresh_time_thrshld = refresh_time_threshold_sec\n",
    "        except NameError:\n",
    "            self.refresh_time_thrshld = None\n",
    "        try:\n",
    "            self.render_comparison_video = render_video_comparison\n",
    "        except NameError:\n",
    "            self.render_comparison_video = False\n",
    "        try:\n",
    "            self.render_video = render_video\n",
    "        except NameError:\n",
    "            self.render_video = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "main.main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
