{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egocentric Video Mapper\n",
    "\n",
    "Before starting make sure you have an available GPU. If you are unsure about whether you have an available GPU or if you want to check which GPU you will be working with run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-72b7ddec-7074-1583-0076-9281c065ae9d)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path to the directory of the uncompressed `Timeseries Data + Scene Video` download from Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_timeseries_dir = \"Path/To/NeonTimeSeriesFolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the path of the corresponding alternative egocentric view recording (please make sure that both videos have the same orientation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_vid_path = \"Path/To/AlternativeVideo.ext\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an output directory where the different output files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Path/To/OutputFolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control how often, in seconds, the egocentric video mapper recalculates new image matches between the videos.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">Note: Higher time thresholds lead to a reduction in computation time, however it may decrease mapping accuracy.</div>\n",
    "\n",
    "If you leave the value at 0 then for every gaze measurement new image matches will be calculated (slowest option but most accurate one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_time_threshold_sec = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "Set the `render_video` variable to True if you want to render the mapped gaze into a copy of the alternative egocentric video (at its original frame rate).\n",
    "\n",
    "Similarly, set the `render_video_comparison` variable to True if you want to render both egocentric videos (Neon Scene and the alternative egocentric camera) side by side showing their respective gaze measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_video = False\n",
    "render_video_comparison = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced configurations\n",
    "\n",
    "Here you can choose which optic flow algorithm will be used for video synchronization, and which image matcher algorithm that will guide the gaze mapping. To check which ones are available in this repo, check `src/pupil_labs/egocentric_video_mapper/optic_flow.py` and `src/pupil_labs/egocentric_video_mapper/feature_matcher.py`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optic_flow_algorithm = \"Lucas-Kanade\"\n",
    "image_matcher = \"Efficient_LOFTR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's run the egocentric video mapper!\n",
    "\n",
    "When everything is finished, you will find the following in the specified output folder:\n",
    "- `alternative_camera_gaze.csv`: The mapped gaze signal. It follows the same structure as gaze.csv, with the frequency of the gaze signal (200Hz) being preserved. This way you can easily integrate it into your existing pipelines.\n",
    "- `alternative_camera_timestamps.csv`: Synchronized UTC timestamps for every alternative egocentric video frame. It follows the same structure as world_timestamps.csv\n",
    "- `alternative_camera-neon_comparison.mp4`: The comparison video showing side by side the Neon Scene camera video and the alternative camera video with their respective gaze signal overlaid.\n",
    "- `alternative_camera_gaze_overlay.mp4`: A copy of the alternative egocentric video with the mapped gaze overlaid.\n",
    "- `neon_optic_flow.csv`: Contains the average optic flow csv for Neon Scene video.\n",
    "- `alternative_optic_flow.csv`: Contains the average optic flow csv for alternative egocentric video.\n",
    "- `egocentric_video_mapper_args.json`: This file contains the different parameters and configurations used to map the gaze signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating optic flow: 100%|██████████| 1262/1262 [00:45<00:00, 27.85it/s]\n",
      "Calculating optic flow: 100%|██████████| 1291/1291 [01:04<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated offset of alternative egocentric video with respect to Neon scene video: 2.016 seconds (Pearson correlation: 0.9192123717590794)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping gaze signal: 100%|██████████| 8155/8155 [01:44<00:00, 77.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaze mapped to alternative camera video saved at /users/sof/test_main/test_note/alternative_camera_gaze.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pupil_labs.egocentric_video_mapper.__main__ as main\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.neon_timeseries_dir = neon_timeseries_dir\n",
    "        self.alternative_vid_path = alternative_vid_path\n",
    "        self.output_dir = output_dir\n",
    "        self.optic_flow_choice = optic_flow_algorithm\n",
    "        self.matcher = image_matcher\n",
    "        self.refresh_time_thrshld = refresh_time_threshold_sec\n",
    "        self.render_comparison_video = render_video_comparison\n",
    "        self.render_video = render_video\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "main.main(args)\n",
    "\n",
    "# Hi, so the Egocentric Video Mapper Alpha Lab is ready for internal testing, I created a Google Colab for use the tool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sof_gazemapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
